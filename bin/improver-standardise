#!/usr/bin/env python
# -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# (C) British Crown Copyright 2017-2018 Met Office.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# * Redistributions of source code must retain the above copyright notice, this
#   list of conditions and the following disclaimer.
#
# * Redistributions in binary form must reproduce the above copyright notice,
#   this list of conditions and the following disclaimer in the documentation
#   and/or other materials provided with the distribution.
#
# * Neither the name of the copyright holder nor the names of its
#   contributors may be used to endorse or promote products derived from
#   this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
"""Script to regrid a source grid to a target grid."""

import numpy as np

import iris
import json

from improver.argparser import ArgParser
from improver.utilities.load import load_cube
from improver.utilities.save import save_netcdf
from improver.utilities.cube_checker import check_cube_not_float64
from improver.utilities.cube_metadata import amend_metadata, delete_attributes


def main():
    """Regrid data from a source cube on to a target grid described by another
       cube."""

    parser = ArgParser(
        description='Standardise a source data cube. Options are to regrid '
                    'with further options to fix float64 data, change '
                    'metatdata, use iris nearest and extrapolation modes as '
                    'part of the regridding process. In addition separate '
                    'standalone checks can be made for float64 data, '
                    'and updating cube metadata'
    )

    parser.add_argument('source_data_filepath', metavar='SOURCE_DATA',
                        help='A cube of data that is to be standardised and '
                             'optionally, fixed for float64 data, regridded '
                             'and meta data changed')

    parser.add_argument("--output_filepath", metavar="OUTPUT_FILE",
                        default=None,
                        help="The output path for the processed NetCDF")

    parser.add_argument("--target_grid_filepath", metavar="TARGET_GRID",
                        help='A cube containing the grid to which '
                             'the source_data is to be regridded.')

    parser.add_argument("--fix_float64", action='store_true', default=False,
                        help="Check and fix cube for float64 data. Without "
                             "this option a warning will be raised if float64 "
                             "data is found but no fix applied.")

    parser.add_argument("--check_float64", action='store_true', default=False,
                       help="Check the cube for float64 data. If float64 "
                            "data is found a warning will be raised but "
                            "no fix applied.")

    parser.add_argument("--change_metadata", action='store_true', default=False,
                        help='Change cube metadata using a specified '
                             'JSON file')

    parser.add_argument("--regrid", action='store_true', default=False,
                        help="regrid cube.....")

    parser.add_argument("--nearest", action='store_true', default=False,
                        help='If True, regridding will be performed using '
                             'iris.analysis.Nearest() instead of Linear().'
                             'Use for less continuous fields, '
                             'e.g. precipitation.')

    parser.add_argument("--extrapolation_mode", default='nanmask',
                        help='Mode to use for extrapolating data into regions '
                             'beyond the limits of the source_data domain. '
                             'Modes are:'
                             'extrapolate - The extrapolation points will '
                             'take their value from the nearest source point. '
                             'nan - The extrapolation points will be be '
                             'set to NaN. '
                             'error - A ValueError exception will be raised, '
                             'notifying an attempt to extrapolate. '
                             'mask  - The extrapolation points will always be '
                             'masked, even if the source data is not a '
                             'MaskedArray. '
                             'nanmask - If the source data is a MaskedArray '
                             'the extrapolation points will be masked. '
                             'Otherwise they will be set to NaN. '
                             'Defaults to nanmask.')

    parser.add_argument("--json_file", metavar="JSON_FILE", default=None,
                        help='Filename for the json file containing required '
                             'changes to the metadata. Defaults to None.')

    args = parser.parse_args()

    # source file data path is a mandatory argument
    source_data = load_cube(args.source_data_filepath)

    # Re-grid with options:
    # if a target grid file, output file path and regrid options have been
    # specified, then regrid optionally applying float64 data check, metadata
    # change, iris nearest and extrapolation mode as required
    if args.target_grid_filepath and args.output_filepath and args.regrid:

        target_grid = load_cube(args.target_grid_filepath)

        if args.fix_float64:
            check_cube_not_float64(source_data, fix=True)
            check_cube_not_float64(target_grid, fix=True)

        # check_cube_not_float64 will raise TypeError if float64 data found
        else:
            check_cube_not_float64(source_data, fix=False)
            check_cube_not_float64(target_grid, fix=False)

        regridder = iris.analysis.Linear(
            extrapolation_mode=args.extrapolation_mode)

        if args.nearest:
            regridder = iris.analysis.Nearest(
                extrapolation_mode=args.extrapolation_mode)

        source_on_target = source_data.regrid(target_grid, regridder)

        target_grid_attributes = (
            {k: v for (k, v) in target_grid.attributes.items()
             if 'mosg__' in k or 'institution' in k})
        amend_metadata(source_on_target, attributes=target_grid_attributes)

        if args.json_file and args.change_metadata:
            with open(args.json_file, 'r') as input_file:
                metadata_dict = json.load(input_file)
            amend_metadata(source_on_target, **metadata_dict)

        save_netcdf(source_on_target, args.output_filepath)

    # Change metadata only option:
    # if output file path and json metadata file specified,
    # change the metadata
    elif args.output_filepath and args.json_file and args.change_metadata:
        with open(args.json_file, 'r') as input_file:
            metadata_dict = json.load(input_file)
        new_data = amend_metadata(source_data, **metadata_dict)
        save_netcdf(new_data, args.output_filepath)

    # Check and fix for float64 data only option:
    elif args.fix_float64 and args.output_filepath:
        check_cube_not_float64(source_data, fix=True)
        save_netcdf(source_data, args.output_filepath)

    # Check but don't fix float64 in source file (only arguments provided are
    # the mandatory source file path and --check_float64)
    # 'check_cube_not_float64' from improver utilities will raise TypeError if
    # float64 data found on the cube.
    elif args.check_float64:
        check_cube_not_float64(source_data, fix=False)
        print('No float64 data found on source file')


if __name__ == "__main__":
    main()
