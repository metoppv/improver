#!/usr/bin/env python
# -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# (C) British Crown Copyright 2017-2018 Met Office.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# * Redistributions of source code must retain the above copyright notice, this
#   list of conditions and the following disclaimer.
#
# * Redistributions in binary form must reproduce the above copyright notice,
#   this list of conditions and the following disclaimer in the documentation
#   and/or other materials provided with the distribution.
#
# * Neither the name of the copyright holder nor the names of its
#   contributors may be used to endorse or promote products derived from
#   this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
"""Script to run spotdata extraction."""
from improver.argparser import ArgParser
import argparse
import glob
import json
import os

from improver.spotdata.ancillaries import get_ancillary_data
from improver.spotdata.main import run_spotdata
from improver.spotdata.read_input import get_method_prerequisites
from improver.spotdata.site_data import ImportSiteData
from improver.spotdata.write_output import WriteOutput
from improver.utilities.load import load_cubelist


def valid_latitude(value):
    """
    Ensures latitude values fall in allowed range.

    Args:
    -----
    value : int/float
        The latitude value to be checked.

    Returns:
    --------
    value : int/float
        The checked latitude value.

    Raises:
    -------
    ArgumentTypeError if latitude is not in valid range.

    """
    value = float(value)
    if value < -90 or value > 90:
        raise argparse.ArgumentTypeError(
            "{} not in range [-90,90]".format(value))
    return value


def valid_longitude(value):
    """
    Ensures longitude values fall in allowed range.

    Args:
    -----
    value : int/float
        The longitude value to be checked.

    Returns:
    --------
    value : int/float
        The checked longitude value.

    Raises:
    -------
    ArgumentTypeError if longitude is not in valid range.

    """
    value = float(value)
    if value < -180 or value > 180:
        raise argparse.ArgumentTypeError(
            "{} not in range [-180,180]".format(value))
    return value


def site_dictionary(latitudes, longitudes, altitudes):
    """
    Format lists of latitudes, latitude and altitudes into site by site
    dictionary entries.

    Args:
    -----
    latitude/longitudes/altitudes : Lists of ints/floats
        Lists of latitudes, longitudes, and altitudes that define the spotdata
        sites.

    Returns:
    --------
    site_properties : dict
        A dictionary of sites, with a latitude, longitude and altitude defined
        for each site.

    """
    if len(latitudes) != len(longitudes):
        raise ValueError('Unequal no. of latitudes and longitudes provided.')

    if altitudes is None:
        altitudes = [None] * len(latitudes)

    if len(latitudes) != len(altitudes):
        raise ValueError('No. of altitudes does not match no. of latitudes/'
                         ' longitudes.')

    site_properties = []
    for site, _ in enumerate(latitudes):
        site_properties.append({
            'latitude': latitudes[site],
            'longitude': longitudes[site],
            'altitude': altitudes[site]
            })

    return site_properties


def main():
    """Load in arguments and start spotdata process."""
    parser = ArgParser(
        description='SpotData : A configurable tool to extract spot-data '
                    'from gridded diagnostics. The method of interpolating '
                    'and adjusting the resulting data can be set by defining '
                    'suitable diagnostics configurations.')

    # Input and output files required.
    parser.add_argument('config_file_path',
                        help='Path to a json file defining the recipes for '
                        'extracting diagnostics at SpotData sites from '
                        'gridded data.')
    parser.add_argument('data_path', type=str,
                        help='Path to a file containing the diagnostic '
                             'to be processed.')
    parser.add_argument('ancillary_path', type=str,
                        help='Path to ancillary (time invariant) data files.')
    parser.add_argument('output_path', type=str,
                        help='Path to which output files should be written.')

    # The diagnostic to be processed.
    parser.add_argument('--diagnostics', type=str, nargs='+', default=None,
                        help='A list of diagnostics that are to be processed. '
                             'If unset, all diagnostics defined in the '
                             'config_file will be produced; e.g. '
                             'temperature wind_speed')

    # File based site definitions.
    parser.add_argument('--site_path', type=str,
                        help='Path to site data file if this is being used '
                             'to choose sites.')
    parser.add_argument('--constants_path', type=str,
                        help='Path to json file containing constants to use '
                             'in SpotData methods.')

    # Run time site definition options.
    parser.add_argument('--latitudes', type=valid_latitude, metavar='(-90,90)',
                        nargs='+',
                        help='List of latitudes of sites of interest.')
    parser.add_argument('--longitudes', type=valid_longitude,
                        metavar='(-180,180)', nargs='+',
                        help='List of longitudes of sites of interest.')
    parser.add_argument('--altitudes', type=float, nargs='+',
                        help='List of altitudes of sites of interest.')

    # Options for speeding up processing.
    parser.add_argument('--multiprocess', action="store_true",
                        help='Process diagnostics using multiprocessing.')

    args = parser.parse_args()

    site_properties = []
    if args.latitudes is not None:
        site_properties = site_dictionary(args.latitudes, args.longitudes,
                                          args.altitudes)

    # Check site data has been provided.
    if args.site_path is None and not site_properties:
        raise ValueError("No SpotData site information has been provided "
                         "from a file or defined at runtime.")

    # If using locations set at command line, set optional information such
    # as site altitude and site_id. If a site definition file is provided it
    # will take precedence.
    if args.site_path is None:
        sites = ImportSiteData('runtime_list').process(site_properties)
    else:
        sites = ImportSiteData('from_file').process(args.site_path)

    # Read in extraction recipes for all diagnostics.
    with open(args.config_file_path, 'r') as input_file:
        diagnostics_from_file = json.load(input_file)

    # Extract only the requested keys from all the available diagnostics
    # that could be processed, as stored within the JSON file.
    if args.diagnostics:
        diagnostics = (
            {key: diagnostics_from_file[key] for key in args.diagnostics})
    else:
        diagnostics = diagnostics_from_file

    # Load ancillary data files; fields that don't vary in time.
    ancillary_data = get_ancillary_data(diagnostics, args.ancillary_path)

    config_constants = None
    if args.constants_path is not None:
        with open(args.constants_path, 'r') as input_file:
            config_constants = json.load(input_file)

    # Create a list of files that will need to be read.
    all_available_files = glob.glob(args.data_path)

    if not all_available_files:
        raise IOError('No data files found in {}.'.format(args.data_path))

    # Loop through the requested diagnostics and load data the essential data
    # and any additional data that is required.
    for diagnostic_key in diagnostics.keys():
        diagnostic_name_in_filename = (
            diagnostics[diagnostic_key]["name_in_filename"])
        files_to_read = (
            [filename for filename in all_available_files
             if diagnostic_name_in_filename in filename])
        if not files_to_read:
            raise IOError('No relevant data files found in {} for {}.'.format(
                args.data_path, diagnostic_name_in_filename))
        # Load cubes into an iris.cube.CubeList.
        diagnostics[diagnostic_key]["data"] = load_cubelist(files_to_read)
        # Check if additional diagnostics are needed (e.g. multi-level data).
        # If required, load into the additional_diagnostics dictionary.
        diagnostics[diagnostic_key]["additional_data"] = (
            get_method_prerequisites(
                diagnostics[diagnostic_key]["interpolation_method"],
                os.path.dirname(args.data_path)))

    resulting_cubes, extrema_cubes = (
        run_spotdata(
            diagnostics, ancillary_data, sites, config_constants,
            use_multiprocessing=args.multiprocess))

    filename = os.path.splitext(os.path.basename(all_available_files[0]))[0]

    if resulting_cubes:
        # Concatenate CubeList into Cube, creating a time DimCoord,
        # and write out.
        cube_out = resulting_cubes.concatenate_cube()
        WriteOutput(
            'as_netcdf', dir_path=args.output_path,
            filename=filename).process(cube_out)

    # If set in the configuration, extract the diagnostic maxima and minima
    # values.
    filepath = all_available_files[0]
    base_filename = os.path.splitext(os.path.basename(filepath))[0]
    for extrema_cubelist in extrema_cubes:
        if extrema_cubelist:
            for extrema_cube in extrema_cubelist:
                filename = "{}_{}".format(base_filename, extrema_cube.name())
                WriteOutput(
                    'as_netcdf', dir_path=args.output_path,
                    filename=filename).process(extrema_cube)


if __name__ == "__main__":
    main()
